0. Выяснить, насколько хорошо отобрали пары документ-категория: какой процент единичек из всей матрицы покрыли этим множеством?
1. Продумать алгоритм близкий по своему смыслу к тому, что видели в описании на форуме kaggle, 
	но с параметрами, которые можно варьировать

	
	подбирать параметры для категорий (локальный подсчет метрики на сайте)
	
	
	Ќайти величину - аналог энтропии в baseline.
	Ќа каком-то примере пар категории-документа, проверить, правильно ли считаетсЯ энтропиЯ, выЯснить, что может быть неправильно
	Џочему алгоритм baseline лучше энтропии?
	Џочему максимальной энтропии не соответствует документ, принадлежащий категории? ЌаходитсЯ ли эта пара алгоритмом из baseline?
	Ќайти несколько примеров пар документ-категориЯ с большой энтропией, но которые не соответствуют друг другу 
	
	entr = 0.657973, doc = 2127362, categ = 29745, in train =0
	entr = 0.631078, doc = 704096, categ = 245246, in train =0
	entr = 0.623891, doc = 921327, categ = 245246, in train =0
	entr = 0.584163, doc = 887888, categ = 245246, in train =0
	entr = 0.580181, doc = 976754, categ = 344539, in train =0
	
30.09.2015	
метрика близости у нас не очень, 
нужен - быстро считающийсЯ эксперимент, демонстрирующий качество метрик

14.10.2015
исправить ошибки и добитьсЯ того же значениЯ, что и baseline
если вдруг получитсЯ сразу, то записать вместо энтропии количество общих категорий и протестировать
 на том методе подбора наилучшего количества категорий
 
 11.11.2015
 1) на исходном множестве запустить knn (без исправлений) и отправить на сайте
 2) если в пункте 1 получитсЯ то, что есть на сайте, отправить baseline по отобранным с помощью энтропии парам документ-категориЯ
 
icpc -std=c++11 -openmp -O2 kaggle.cpp -g1 -traceback -o kaggle


на трейне - 0.13
чистый алгоритм - 0.17952
с модификацией отбрасываниЯ уникальных терминов - 0.18040

25.11.2015
1) Џредсказание по энтропии с выбором наилучшего количества предсказываемых документов длЯ категории отправить на kaggle, должно получитьсЯ 0.26 
(ну или во всЯком случае 0.2078 - как получилось в трейн множестве)
2) если получитсЯ первый пункт, то как сделать еще лучше? например, скрестить энтропию с тф-идф, кнн
вспомнить, как считали энтропию. Ќадо не стрелЯть, тратЯ патроны, а попробовать прицелитьсЯ. Ќесколько вариантов, как можно использовать и почему они могут быть лучше в комбинации
Џо какому принципу отобраны категории длЯ теста?
3) сделать функцию подсчета энтропии с параметрами


что показывает энтропия? - выш 0.09577 подняться почему-то не удается
возможные проблемы: 1) неправильное чтение - но оно же используется и в том, что дает 0.18040   - потестить чтение тестового файла, сравнить функции - НЕТ, ОНО ПРАВИЛЬНОЕ
					2) ошибка в алгоритме построения тестового множества  - еще раз проверить досконально  - НЕ ОТЛИЧАЕТСЯ ОТ ПОСТРОЕНИЯ ТРЕЙНА
					3) результат 0,2078 недостижим, т.к. для него использовалось заглядывание в будущее и мб какие-то другие причины  - что делать???
					
					
дз
0. все в git

09.12.2015
провели классный эксперимент.
 на kaggle плохие результаты, т.к. переобучение. Построили график MaF(барьер), и он жутко изрезан, причём резкие обрывы наблюдаются только с левой стороны. Нужно стараться брать лучшее 
 значение барьера чуть больше, чем оно есть на одной выборке. 
 Правильный выход из создавшегося ужасного положения: 

Проводим серию вычисл экспериментов. 
